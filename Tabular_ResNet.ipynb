{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlPsjdEunI1SCXuGKpCACG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://www.kaggle.com/code/ritvik1909/tabular-resnet)"
      ],
      "metadata": {
        "id": "okcLFrMWjHsp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OWP435rhU-E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers as L\n",
        "from tensorflow_addons.activations import sparsemax\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import joblib\n",
        "\n",
        "pd.options.display.max_columns = 300\n",
        "\n",
        "data = pd.read_csv('./data/train.csv')\n",
        "data = data.drop_duplicates(subset=data.columns[1:]).reset_index(drop=True)\n",
        "print(data.shape)\n",
        "data.head()\n",
        "\n",
        "test = pd.read_csv('./data/test.csv')\n",
        "print(test.shape)\n",
        "X_test = test.drop(['row_id'], axis=1)\n",
        "\n",
        "X = data.drop(['row_id', 'target'], axis=1)\n",
        "y = pd.get_dummies(data['target'])\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "class DataConfig:\n",
        "    def __init__(self, numeric_feature_names, categorical_features_with_vocabulary):\n",
        "        self.NUMERIC_FEATURE_NAMES = numeric_feature_names\n",
        "        self.CATEGORICAL_FEATURES_WITH_VOCABULARY = categorical_features_with_vocabulary\n",
        "        self.CATEGORICAL_FEATURE_NAMES = list(self.CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
        "        self.FEATURE_NAMES = self.NUMERIC_FEATURE_NAMES + self.CATEGORICAL_FEATURE_NAMES\n",
        "\n",
        "from tensorflow.data import Dataset\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    @classmethod\n",
        "    def from_df(cls, X, y=None, batch_size=1024):\n",
        "        return (\n",
        "            Dataset.from_tensor_slices(({col: X[col].values.tolist() for col in X.columns}, y.values.tolist())).batch(\n",
        "                batch_size\n",
        "            )\n",
        "            if y is not None\n",
        "            else Dataset.from_tensor_slices({col: X[col].values.tolist() for col in X.columns}).batch(batch_size)\n",
        "        )\n",
        "\n",
        "def get_inputs(config):\n",
        "    return {\n",
        "        feature_name: L.Input(\n",
        "            name=feature_name,\n",
        "            shape=(),\n",
        "            dtype=(tf.float32 if feature_name in config.NUMERIC_FEATURE_NAMES else tf.string),\n",
        "        )\n",
        "        for feature_name in config.FEATURE_NAMES\n",
        "    }\n",
        "\n",
        "def encode_inputs(inputs, config, use_embeddings=False, embedding_dim=32, prefix=\"\", concat_features=False):\n",
        "    cat_features = []\n",
        "    num_features = []\n",
        "    for feature_name in inputs:\n",
        "        if feature_name in config.CATEGORICAL_FEATURE_NAMES:\n",
        "            vocabulary = config.CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "            lookup = L.StringLookup(\n",
        "                vocabulary=vocabulary,\n",
        "                mask_token=None,\n",
        "                num_oov_indices=0,\n",
        "                output_mode=\"int\" if use_embeddings else \"binary\",\n",
        "                name=f\"{prefix}{feature_name}_lookup\",\n",
        "            )\n",
        "            if use_embeddings:\n",
        "                encoded_feature = lookup(inputs[feature_name])\n",
        "                embedding = L.Embedding(\n",
        "                    input_dim=len(vocabulary),\n",
        "                    output_dim=embedding_dim,\n",
        "                    name=f\"{prefix}{feature_name}_embeddings\",\n",
        "                )\n",
        "                encoded_feature = embedding(encoded_feature)\n",
        "            else:\n",
        "                encoded_feature = lookup(\n",
        "                    L.Reshape((1,), name=f\"{prefix}{feature_name}_reshape\")(inputs[feature_name])\n",
        "                )\n",
        "            cat_features.append(encoded_feature)\n",
        "        else:\n",
        "            encoded_feature = L.Reshape((1,), name=f\"{prefix}{feature_name}_reshape\")(inputs[feature_name])\n",
        "            num_features.append(encoded_feature)\n",
        "\n",
        "    features = (\n",
        "        L.Concatenate(name=f\"{prefix}inputs_concatenate\")(cat_features + num_features)\n",
        "        if concat_features\n",
        "        else (cat_features, num_features)\n",
        "    )\n",
        "\n",
        "    return features\n",
        "\n",
        "class TabularResNetConfig:\n",
        "    def __init__(\n",
        "        self, num_outputs, out_activation, hidden_units, dropout_rate=0.3, use_embeddings=True, embedding_dim=32\n",
        "    ):\n",
        "        self.NUM_OUT = num_outputs\n",
        "        self.OUT_ACTIVATION = out_activation\n",
        "        self.HIDDEN_UNITS = hidden_units\n",
        "        self.DROPOUT_RATE = dropout_rate\n",
        "        self.USE_EMBEDDINGS = use_embeddings\n",
        "        self.EMBEDDING_DIM = embedding_dim\n",
        "\n",
        "def skip_connection(inputs, units, i, dropout_rate):\n",
        "    inputs = L.Dense(units, name=f\"block_{i+1}_1_dense\")(inputs)\n",
        "    inputs = L.BatchNormalization(name=f\"block_{i+1}_1_b_norm\")(inputs)\n",
        "    inputs = L.ReLU(name=f\"block_{i+1}_1_relu\")(inputs)\n",
        "    inputs = L.Dropout(dropout_rate, name=f\"block_{i+1}_1_dropout\")(inputs)\n",
        "    x = L.Dense(units, name=f\"block_{i+1}_2_dense\")(inputs)\n",
        "    x = L.BatchNormalization(name=f\"block_{i+1}_2_b_norm\")(x)\n",
        "    x = L.ReLU(name=f\"block_{i+1}_2_relu\")(x)\n",
        "    x = L.Dropout(dropout_rate, name=f\"block_{i+1}_2_dropout\")(x)\n",
        "    return L.Add(name=f\"skip_{i+1}\")([inputs, x])\n",
        "\n",
        "\n",
        "class TabularResNet:\n",
        "    @classmethod\n",
        "    def from_config(cls, data_config, model_config, name):\n",
        "        inputs = get_inputs(data_config)\n",
        "        features = encode_inputs(\n",
        "            inputs,\n",
        "            data_config,\n",
        "            use_embeddings=True,\n",
        "            embedding_dim=model_config.EMBEDDING_DIM,\n",
        "            prefix=\"\",\n",
        "            concat_features=True,\n",
        "        )\n",
        "        for i, units in enumerate(model_config.HIDDEN_UNITS):\n",
        "            features = skip_connection(features, units, i, model_config.DROPOUT_RATE)\n",
        "        outputs = L.Dense(\n",
        "            units=model_config.NUM_OUT,\n",
        "            activation=model_config.OUT_ACTIVATION,\n",
        "            name=\"outputs\",\n",
        "        )(features)\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
        "        return model\n",
        "\n",
        "data_config = DataConfig(\n",
        "    numeric_feature_names=X.columns.tolist(), categorical_features_with_vocabulary={}\n",
        ")\n",
        "model_config = TabularResNetConfig(num_outputs=len(y.columns), out_activation='softmax', hidden_units=[128, 64, 32])\n",
        "\n",
        "blank_model = TabularResNet.from_config(data_config, model_config, name='tab_resnet')\n",
        "blank_model.summary()\n",
        "\n",
        "MAX_EPOCHS  = 50\n",
        "\n",
        "get_callbacks = lambda : [\n",
        "    keras.callbacks.EarlyStopping(min_delta=1e-4, patience=3, verbose=1, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1)\n",
        "]\n",
        "\n",
        "preds = []\n",
        "\n",
        "for fold, (train_index, valid_index) in enumerate(skf.split(X, data['target'])):\n",
        "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n",
        "    X_valid = pd.DataFrame(scaler.transform(X_valid), columns=X.columns)\n",
        "    x_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
        "\n",
        "    data_train = DataLoader.from_df(X_train, y_train, batch_size=512)\n",
        "    data_valid = DataLoader.from_df(X_valid, y_valid, batch_size=512)\n",
        "    data_test = DataLoader.from_df(x_test, batch_size=512)\n",
        "\n",
        "    model = TabularResNet.from_config(data_config, model_config, name=f'tab_resnet_fold_{fold}')\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy']\n",
        "    )\n",
        "    model.fit(\n",
        "        data_train, validation_data=data_valid, callbacks=get_callbacks(),\n",
        "        epochs=MAX_EPOCHS\n",
        "    )\n",
        "    preds.append(model.predict(data_test))\n",
        "\n",
        "submissions = pd.read_csv('./data/sample_submission.csv')\n",
        "submissions['target'] = pd.DataFrame(\n",
        "    np.array([arr for arr in preds]).mean(axis=0),columns=y.columns\n",
        ").idxmax(axis=1).values.tolist()\n",
        "submissions.to_csv('preds.csv', index=False)"
      ]
    }
  ]
}